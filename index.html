<!DOCTYPE html>
<html lang="en" dir="ltr">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content>
  <meta name="author" content>
  <link rel="shortcut icon" type="image/png" href="/assets/img/favicon.png">
  <title>Holly Huey</title>
  <script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <link href='https://fonts.googleapis.com/css?family=Roboto' rel='stylesheet' type='text/css'>
  <!-- /* Large devices: desktops, 992px, and up */ -->
  <link rel="stylesheet" type="text/css" href="css/stylesheet-desktop.css" media="only screen and (max-width: 1920px)">
  <link rel="stylesheet" type="text/css" href="css/stylesheet-desktop-medium.css" media="only screen and (max-width: 1000px)">
  <!-- /* Large devices: desktops, 992px, and up */ -->
  <link rel="stylesheet" type="text/css" href="css/stylesheet-tablet.css" media="only screen and (max-width: 768px)">
  <!-- /* Large devices: desktops, 992px, and up */ -->
  <link rel="stylesheet" type="text/css" href="css/stylesheet-mobile.css" media="only screen and (max-width: 576px)">
  <link rel="stylesheet" href="css/stylesheet-animated-hero.css">
  <link rel="shortcut icon" type="image/png" href="assets/norm-site-icons/favicon.png"> 
  <script type="text/javascript" src="js/animation.js"></script>
</head>

<body>
  <!-- Landing Page -->
  <div id="background">
    <div id="landing_background">
      <div id="landing-page-content">
        <div id="hero-content" class="landing-text">
          <div>
            <p><span id="hero-header" class="in-down">Holly Huey</span><span class="period-dot in-down">.</span>
            </p>
          </div>
          <div id="hero-description" class="in-down">
            <p>Applied Research Scientist. Evaluating GenAI image & video models by benchmarking human visual content creation. 
<!-- Benchmarking human behavior & communication to improve generative AI visualization tools. -->
            </p>
          </div>
          <div id="hero-navigation-bar" class="text in-down">
            <nav>
              <ul>
                <li><a href="https://hollyhuey.github.io/#about" class="nav text">about</a></li>
                <li><a href="https://hollyhuey.github.io/#research" class="nav text">research</a></li>
                <li><a href="https://hollyhuey.github.io/#publications" class="nav text">publications</a></li>
                <li><a href="assets/cv/hhuey_resume_edited.pdf" class="nav text" target="_blank">resume</a></li>
                <!-- <li><a href="assets/cv/HollyHuey_cv2019.pdf" class="nav text" target="_blank">cv</a>&<a href="assets/cv/hhuey_resume_edited.pdf" class="nav text" target="_blank">resume</a></li> -->
              </ul>
            </nav>
          </div>
        </div>
        <a href="https://hollyhuey.github.io/#about">
          <img src="assets/norm-site-icons/scroll-down-arrow.png" class="btn-continue animated bounce">
        </a>
      </div>
    </div>

    <!-- About -->
    <div id="wrapper" class="text">
      <div id="headshot-content">
        <headshot-span>
          <img id="headshot" src="assets/photos/headshot.png">
        </headshot-span>
      </div>
      <div class="line-transition">
        <div id="bio-header"><a id="about">
            <!-- https://stackoverflow.com/questions/15481911/linking-to-a-specific-part-of-a-web-page -->
            <div id="typing-wrapper"><typy>How do we use visual media to <span id="typing-text"></span></typy></div>
          </a></div>
        <div class="bio-text-desc">
        <div style="margin-bottom: 2em">
          <p>Hi there! I'm a 5th-year PhD Candidate in Experimental Psychology at the 
            University of California San Diego.
            <br>
            I'm advised by Dr. Judith Fan @ 
            <a href="https://cogtoolslab.github.io/" target="_blank" class="external">The Cognitive Tools Lab</a>.</p>

          <p>I specialize in large-scale crowdsourcing techniques to benchmark how people create, interpret, and interact with visual content.</p> 

            <p>In my PhD, I've conducted a wide range of visual production tasks by custom coding online web experiments 
            (using <a href="https://www.jspsych.org/latest/" class="external" target="_blank">jsPsych</a>) 
            and comparing people's behaviors against various heuristic models and LLMs.
            I have also worked with Adobe Research in 3 internships evaluating (1) GenAI <i>image</i> output against human 
            expectations of model reasoning and (2) GenAI <i>video</i> editing. 

            <!-- I've also conducted developmental research using drawing to study children's conceptual development of visual object concepts. </p> -->
            
            <p>My approach leverages quantitative & qualitative methods to improve human-centered tools related to:
            <i>text-to-visual systems, audience design, object identification, semantic segmentation</i></p>

            <!-- <p>Here's my latest <a href="assets/cv/hhuey_resume_edited.pdf" class="external" target="_blank">resume</a>.</p> -->
          </div>  
          <div>
          <p class="text-footer"><b>Bio:</b>
            I grew up on <a href="https://en.wikipedia.org/wiki/Whidbey_Island" class="external" target="_blank">Whidbey Island, WA</a>. 
            As an introverted teen on a rural island, I spent most of my time sketching and hiding away in art studios. 
            After 2 art apprenticeships and double majoring in Philosophy and History of Mathematics & Sciences from 
            <a href="https://www.sjc.edu/" class="external" target="_blank">St. John's College</a>, 
            I became fascinated by how novel visualizations have catalyzed and served to communicate many major scientific discoveries 
            (e.g., Cartesian coordinate system, periodic table, Vitruvian man). 
            Despite this critical role of visual media in human innovation, relatively little is known about how people communicate 
            in visual form. This question continues to inspire my research.</p>
          </div>
        </div> <!-- bio-text-desc -->
      </div>

      <!-- Research -->
      <div id="bio-research" class="line-transition"><a id="research">
          <h1>research domains</h1>
        </a>
        <div class="grid-container">
          <div>
            <h3>
              <center>Visualizations</center>
            </h3>
            <div>
              <img src="assets/figures/vab_hero.png" class="research-figures">
            </div> 
            <div class="bio-text-desc research-text">By crowdsourcing & analyzing large-scale sketch datasets by humans 
              and GenAI sketching algorithms, I evaluate the difference between human and machine 
              sketch behavior under different task constraints.

              <p class="text-footer">Our <a href="assets/studies/mukherjee_neurips_2023.pdf" target="_blank" class="external">NeurIPS 2023 paper</a>
              collected >90K drawings across 128 object categories and compared how well 17 visions models could recognize 
              the sketched objects by comparison to human viewers.</p>
            </div>
            <!-- <div class="slideshow-container">
              <div class="mySlides fade">
                <img src="assets/figures/neurips_polysemy.png" class="research-figures">
              </div>
              
              <div class="mySlides fade">
                <img src="assets/figures/neurips_sketchgallery.png" class="research-figures">
              </div> 
              
              <div style="text-align:center">
                <span class="dot_4" onclick="currentSlide_2(1)"></span> 
                <span class="dot_4" onclick="currentSlide_2(2)"></span> 
              </div>
              </div> -->
            <!-- <div class="bio-text-desc research-text">Since the first cave art, drawings have been a window into how people 
              organize their knowledge into symbolic representation. 
              By collecting & analyzing large-scale drawing datasets by humans, AI models, and children, 
              my work aims to investigates how people interpret the visual objects and scenes in their environments.</div>

              <div id="text-footer">
                *In <a href="assets/studies/mukherjee_neurips_2023.pdf" target="_blank" class="external">recent work accepted to NeurIPS</a>, my collaborators and I crowdsourced & analyzed >90K drawings generated by both humans and generative sketch AI! 
              </div> -->
          </div>
          <div>
            <h3>
              <center>IMAGES</center>
            </h3>
            <div>
              <img src="assets/figures/model_reasoning_hero2.png" class="research-figures">
            </div> 
            <div class="bio-text-desc research-text">
              Text-to-image generation models provide new opportunities for content creators to quickly and flexibly generate images. 
              To systematically measure and compare the consistent errors that models make, I developed a series of behavioral tasks to 
              crowdsource human annotators and generate quality scores for large-scale image datasets.
            </div>
            <!-- <div class="slideshow-container">
              <div class="mySlides_2 fade">
                <img src="assets/figures/causaldraw_task.png" class="research-figures">
              </div>
              
              <div class="mySlides_2 fade">
                <img src="assets/figures/causaldraw_results.png" class="research-figures">
              </div> 
              
              <div style="text-align:center">
                <span class="dot_2" onclick="currentSlide_2(1)"></span> 
                <span class="dot_2" onclick="currentSlide_2(2)"></span> 
              </div>
              </div> -->
            <!-- <div class="bio-text-desc research-text">
              Visual explanations are at the core of human innovation & design.
              How do people's goals to convey higher-level knowledge about physical
              mechanism shift how they produce diagrams? 
              This line of work investigates how people prioritize different kinds 
              of semantic information and what behavioral impact their representational choices have on viewers.</div> -->
          </div>
          <div>
            <h3>
              <center>VIDEO</center>
            </h3>
            <div>
              <img src="assets/figures/video_broll_hero.png" class="research-figures">
            </div> 
            <div class="bio-text-desc research-text">What makes videos compelling?
              In my summer 2023 and 2024 internships at Adobe Research, I evaluated how people generate and edit videos depending on their content creation goals.
              I conducted qualitative interviews for discovering user needs, as well as custom coded online experiments for evaluating user behaviors related to video production. 

              <p class="text-footer">Our <a href="https://camps.aptaracorp.com/ACM_PMS/PMS/ACM/CC24/53/e0c1f3bf-0e3f-11ef-8182-16bb50361d1f/OUT/cc24-53.html" target="_blank" class="external">Creativity & Cognition 2024 paper</a> 
              evaluated how people prioritize different visual concepts to illustrate as B-Roll images 
              depending on their goals to make entertaining or informative videos. This work crowdsourced >900 human annotators 
              and computationally compared their concept selection behavior against heuristic & GPT models.</p>
            </div>
            <!-- <div class="slideshow-container">
              <div class="mySlides_3 fade">
                <img src="assets/figures/davinci_task.png" class="research-figures">
              </div>
              
              <div class="mySlides_3 fade">
                <img src="assets/figures/davinci_results.png" class="research-figures">
              </div> 
              
              <div style="text-align:center">
                <span class="dot_3" onclick="currentSlide_3(1)"></span> 
                <span class="dot_3" onclick="currentSlide_3(2)"></span> 
              </div>
              </div> -->
            <!-- <div class="bio-text-desc research-text">In our modern era, data visualizations have become a critical tool to understand large-scale information 
              & reveal predictive statistical trends. However, even more simple graphs 
              (barcharts, linecharts, scatterplots) require years of education to understand & make them.
              By analyzing how people answer different questions about graphs, my work aims to identify different 
              ways of visualizing data can lead to more effective graph design & comprehension.
              </div> -->
          </div>
        </div>
        <div>
          <div class="bio-text-desc extra-research">I have also explored & published work in other domains: 
            language, social pragmatics (theory of mind), navigation, causal perception, symbolic reasoning, and developmental psychology.
            See below publications or my <a href="https://scholar.google.com/citations?user=FSRbchUAAAAJ&hl=en" class="external" target="_blank">Google Scholar</a> for papers in these areas.
          </div>
        </div>
      </div>

      <!-- Publications -->
      <div id="bio-publications"><a id="publications">
          <h1>publications</h1>
        </a></div>

      <ul class="publications">
        <li class="text publication-text">
          <div class="sub-pubs">journal articles</div>
        </li>

        <li class="text indiv-pub-text">
          Long, B., Fan, J., <b>Huey, H.,</b> Chai, R., & Frank, M. C. (2024).
          Parallel developmental changes in children's production and recognition of line drawings of visual concepts.
          <i>Nature Communications.</i>
          <br>
          <a href="assets/studies/long_etal_2024.pdf" target="_blank" class="external">Paper</a>&nbsp;&nbsp;
        </li>

        <li class="text indiv-pub-text">
          <b>Huey, H.,</b> Lu, X., Walker, C.M., & Fan, J.E. (2023).
          Explanatory drawings prioritize functional properties at the expense of visual fidelity. <i>Cognition.</i>
          <br>
          <a href="assets/studies/causaldraw_cognition.pdf" target="_blank" class="external">Paper</a>&nbsp;&nbsp;
          <a href="assets/studies/davinci_cogsci2023.pdf" target="_blank" class="external">Poster</a>&nbsp;&nbsp;
          <a href="https://www.sciencedirect.com/science/article/pii/S0010027723000483?via%3Dihub" target="_blank" class="external">Publisher's Page</a>
        </li>

        <li class="text indiv-pub-text">
          <b>*Huey, H.,</b>*Jordan, M., & Dillon,. M.R. (2023).
          Shortest path problems on different geometric surfaces: Reasoning about linearity through development. <i>Developmental Psychology.</i>
          <br>
          <a href="assets/studies/HueyJordan_etal_2023.pdf" target="_blank" class="external">Paper</a>&nbsp;&nbsp;
          <a href="https://psycnet.apa.org/record/2023-35044-001" target="_blank" class="external">Publisher's Page</a>
        </li>

        <li class="text indiv-pub-text">
          Aboody, R., <b>Huey, H.,</b> & Jara-Ettinger, J. (2022).
          Preschoolers decide who is knowledgeable, who to inform, and who to trust via a causal understanding of how knowledge
          relates to action. <i>Cognition.</i>
          <br>
          <a href="assets/studies/predict-observe.pdf" target="_blank" class="external">Paper</a>&nbsp;&nbsp;
          <a href="https://www.sciencedirect.com/science/article/pii/S0010027722002001?via%3Dihub" target="_blank" class="external">Publisher's Page</a>
        </li>

        <li class="text indiv-pub-text">
          Jara-Ettinger, J., Floyd, S., <b>Huey, H.,</b> Tenenbaum, J.B., & Schulz, L. (2019).
          Social pragmatics: Four and five-year-olds rely on commonsense psychology to
          resolve referential ambiguities. <i>Child Development.</i>
          <br>
          <a href="assets/studies/Jara-Ettinger_et_al2019_Child_Development.pdf" target="_blank" class="external">Paper</a>&nbsp;&nbsp;
          <a href="https://srcd.onlinelibrary.wiley.com/doi/10.1111/cdev.13290" target="_blank" class="external">Publisher's Page</a>
        </li>
        <br>

        <li class="text publication-text">
          <div class="sub-pubs">peer-reviewed conference proceedings & posters</div>
        </li>

        <li class="text indiv-pub-text">
          <b>Huey, H.,</b> Leake, M., Aneja, D., Fisher, M., and Fan, J.E. (2024).
          How do video content creation goals impact which concepts people prioritize for generating B-roll imagery?
          <i>Creativity and Cognition</i>.
          Chicago, Illinois: Association for Computing Machinery
          <br>
          <a href="https://camps.aptaracorp.com/ACM_PMS/PMS/ACM/CC24/53/e0c1f3bf-0e3f-11ef-8182-16bb50361d1f/OUT/cc24-53.html" target="_blank" class="external">Paper</a>&nbsp;&nbsp;
          <a href="assets/studies/huey_broll_cc_2024.pdf" target="_blank" class="external">Poster</a>
        </li>

        <li class="text indiv-pub-text">
          *Mukherjee, K., <b>*Huey, H.,</b> *Lu, X., Vinker, Y., Aguina-Kang, R., and Fan, J.E. (2023).
          SEVA: Leveraging sketches to evaluate alignment between human and machine visual abstraction.
          <i>Advances in Neural Information Processing Systems, Datasets and Benchmarks Track, 2023</i>.
          New Orleans, Lousiana: NeurIPS
          <br>
          <a href="assets/studies/mukherjee_neurips_2023.pdf" target="_blank" class="external">Paper</a>&nbsp;&nbsp;
        </li>

        <li class="text indiv-pub-text">
          <b>*Huey, H.,</b> *Oey, L.A., Lloyd, H.S., and Fan, J.E. (2023).
          How do communicative goals guide which data visualizations people think are effective?
          <i>Proceedings of the 45th Annual Conference of the Cognitive Science Society.</i>
          Sydney, Australia: Cognitive Science Society.
          <br>
          <a href="assets/studies/hueyoey_cogsci_2023.pdf" target="_blank" class="external">Paper</a>&nbsp;&nbsp;
          <a href="assets/studies/davinci_cogsci2023.pdf" target="_blank" class="external">Poster</a>
        </li>

        <li class="text indiv-pub-text">
          *Mukherjee, K., <b>*Huey, H.,</b> *Lu, X., Vinker, Y., Aguina-Kang, R., and Fan, J.E. (2023).
          Evaluating machine comprehension of sketch meaning at different levels of abstraction.
          <i>Proceedings of the 45th Annual Conference of the Cognitive Science Society.</i>
          Sydney, Australia: Cognitive Science Society.
          <br>
          <a href="assets/studies/mukherjee_cogsci_2023.pdf" target="_blank" class="external">Paper</a>&nbsp;&nbsp;
          <a href="assets/studies/VAB_CogSci_2023_poster.pdf" target="_blank" class="external">Poster</a>
        </li>

        <li class="text indiv-pub-text">
          Lloyd, H.S., <b>*Huey, H.,</b> Brockbank, E.,  Padilla, L., and Fan, J.E. (2023).
          What is graph comprehension and how do you measure it?
          <i>Proceedings of the 45th Annual Conference of the Cognitive Science Society.</i>
          Sydney, Australia: Cognitive Science Society.
          <br>
          <a href="assets/studies/GCB_cogsci2023_abstract.pdf" target="_blank" class="external">Abstract</a>&nbsp;&nbsp;
          <a href="assets/studies/gcb_portrait.pdf" target="_blank" class="external">Poster</a>
        </li>

        <li class="text indiv-pub-text">
          <b>*Huey, H.,</b> *Long, B., Yang, J., George, K., and Fan, J.E. (2022).
          Developmental changes in the semantic part structure of drawn objects.
          <i>Proceedings of the 44th Annual Conference of the Cognitive Science Society.</i>
          Toronto, Canada: Cognitive Science Society.
          <br>
          <a href="assets/studies/hueylong_cogsci_2022.pdf" target="_blank" class="external">Paper</a>&nbsp;&nbsp;
          <a href="assets/studies/cogsci_kiddraw_annotations_2022_FINALPOSTER_6.pdf" target="_blank" class="external">Poster</a>
        </li>

        <li class="text indiv-pub-text">
          Nagabandi, M., Yang, J., <b>Huey, H.,</b> Fan, J.E. (2022).
          Decomposing objects into parts from vision and language.
          <i>Proceedings of the 44th Annual Conference of the Cognitive Science Society.</i>
          Toronto, Canada: Cognitive Science Society.
          <br>
          <a href="assets/studies/partnaming_cogsci2023_abstract.pdf" target="_blank" class="external">Abstract</a>&nbsp;&nbsp;
        </li>

        <li class="text indiv-pub-text">
          <b>Huey, H.,</b> Walker, C.M., & Fan, J.E. (2021).
          How do the semantic properties of visual explanations guide causal inference?
          <i>Proceedings of the 43rd Annual Conference of the Cognitive Science Society.</i>
          (Virtual Meeting) Vienna, Austria: Cognitive Science Society.
          <br>
          <a href="assets/studies/semanticproperties_510.pdf" target="_blank" class="external">Paper</a>&nbsp;&nbsp;
          <a href="assets/studies/cogsci_poster_2021_final.pdf" target="_blank" class="external">Poster</a>&nbsp;&nbsp;
          <a href="assets/studies/cogsci_poster_run2.mp4" target="_blank" class="external">Video</a>
        </li>

        <li class="text indiv-pub-text">
          Aboody, R., <b>Huey, H.,</b> & Jara-Ettinger, J. (2018). Success does
          not imply knowledge: Preschoolers believe that accurate predictions reveal
          prior knowledge, but accurate observations do not.
          <i>Proceedings of the 40th Annual Conference of the Cognitive Science Society.</i>
          <br>
          <a href="assets/studies/Aboody_Huey_Jara-Ettinger_2018_CogSci.pdf" target="_blank" class="external">Paper</a>
        </li>

        <li class="text indiv-pub-text">
          Aboody, R., <b>Huey, H.,</b> & Jara-Ettinger, J. (2017).
          Success does not imply knowledge: Preschoolers believe that accurate predictions reveal prior knowledge,
          but accurate observations do not.
          <i>Poster presented at the Cognitive Development Society's Bi-Annual Meeting 2017.</i>
          <br>
          <a href="assets/studies/CDS_poster_2017.pdf" target="_blank" class="external">Poster</a>
        </li>

        <br>

        <li class="text publication-text">
          <div class="sub-pubs">workshop presentations</div>
        </li>

        <li class="text indiv-pub-text"> 
          Fan, J.E., Mukherjee, K., <b>Huey, H.,</b> Hebart, M.A., & Bainbridge. W.A. (2023).
          THINGS-drawings: A large-scale dataset containing human sketches of 1,854 object concepts.
          <i>Journal of Vision.</i>
          St. Pete Beach, FL: Vision Sciences Society          
          <br>
          <a href="https://https://jov.arvojournals.org/article.aspx?articleid=2792322" class="external">Talk abstract</a>&nbsp;&nbsp;
        </li>

        <li class="text indiv-pub-text"> 
          Mukherjee, K., <b>Huey, H.,</b> Rogers, T., & Fan. J.E. (2022).
          From Images to Symbols: Drawing as a Window into the Mind.
          <i>Proceedings of the 44th Annual Conference of the Cognitive Science Society.</i>
          Toronto, Canada: Cognitive Science Society.
          <b>Co-organizer of workshop & co-designer of website.</b>
          <br>
          <a href="https://images2symbols.github.io/" class="external">Website</a>&nbsp;&nbsp;
          <a href="assets/studies/images2symbols.pdf" target="_blank" class="external">Workshop Overview</a>
        </li>

        <li class="text indiv-pub-text">
          Pitt, B., <b>Huey, H.,</b> Jordan, M., Hart, Y., Dillon, M.R.,
          Bottini, R., Carstensen, A., Boni, I., Piantadosi, S., Gibson, E., Marghetis, T.,
          Holmes, K.J., Star-Lack, M., & Chacon, S., (2022).
          Dimensions of Diversity in Spatial Cognition: Culture, Context, Age, and Ability.
          <i>Proceedings of the 44th Annual Conference of the Cognitive Science Society.</i>
          Toronto, Canada: Cognitive Science Society.
          <b>Invited speaker.</b>
          <br>
          <a href="assets/studies/diverse_spatialcognition.pdf" target="_blank" class="external" role="button">Workshop Overview</a>
        </li>
      </ul>
    </div>
  </div>

  <!-- Contact Icons -->
  <div id="social-wrappper" class="site-footer">
    <button type="button" class="btn social-button">
      <span class="mailto-wrapper">
        <a href="mailto:hhuey@ucsd.edu" class="mailto-link">
          <img src="assets/social-icons/email-outline.svg" class="social-icons">
        </a>
      </span>
    </button>
    <button type="button" class="btn social-button">
      <a href="https://www.linkedin.com/in/holly-huey/">
        <img src="assets/social-icons/linkedin-outline.svg" class="social-icons">
      </a>
    </button>
    <button type="button" class="btn social-button">
      <a href="https://github.com/hollyhuey">
        <img src="assets/social-icons/github-outline.svg" class="social-icons">
      </a>
    </button>
    <button type="button" class="btn social-button">
      <a href="https://twitter.com/hollyahuey">
        <img src="assets/social-icons/twitter-outline.png" class="social-icons">
      </a>
    </button>
    <button type="button" class="btn social-button">
      <a href="https://scholar.google.com/citations?user=FSRbchUAAAAJ&hl=en">
        <img src="assets/social-icons/google-scholar.svg" class="social-icons">
      </a>
    </button>
  </div>
  <div id="design-credit">
    <p style="opacity: .5;">Designed by Holly Huey. Photo credit: Petrina Chan (headshot) & Terra Huey (headshot art)</p>
  </div>

  <!-- note: for some reason, there's a scoping error if this is moved to animation.js -->
  <script>
    let slideIndex = 1;
    showSlides(slideIndex);
    
    function currentSlide(n) {
      showSlides(slideIndex = n);
    }
    
    function showSlides(n) {
      let i;
      let slides = document.getElementsByClassName("mySlides");
      let dots = document.getElementsByClassName("dot_4");
      if (n > slides.length) {slideIndex = 1}    
      if (n < 1) {slideIndex = slides.length}
      for (i = 0; i < slides.length; i++) {
        slides[i].style.display = "none";  
      }
      for (i = 0; i < dots.length; i++) {
        dots[i].className = dots[i].className.replace(" active", "");
      }
      slides[slideIndex-1].style.display = "block";  
      dots[slideIndex-1].className += " active";
    }

    let slideIndex_2 = 1;
    showSlides_2(slideIndex_2);
    
    function currentSlide_2(n) {
      showSlides_2(slideIndex_2 = n);
    }
    
    function showSlides_2(n) {
      let i;
      let slides = document.getElementsByClassName("mySlides_2");
      let dots = document.getElementsByClassName("dot_2");
      if (n > slides.length) {slideIndex_2 = 1}    
      if (n < 1) {slideIndex_2 = slides.length}
      for (i = 0; i < slides.length; i++) {
        slides[i].style.display = "none";  
      }
      for (i = 0; i < dots.length; i++) {
        dots[i].className = dots[i].className.replace(" active", "");
      }
      slides[slideIndex_2-1].style.display = "block";  
      dots[slideIndex_2-1].className += " active";
    }

    let slideIndex_3 = 1;
    showSlides_3(slideIndex_3);
    
    function currentSlide_3(n) {
      showSlides_3(slideIndex_3 = n);
    }
    
    function showSlides_3(n) {
      let i;
      let slides = document.getElementsByClassName("mySlides_3");
      let dots = document.getElementsByClassName("dot_3");
      if (n > slides.length) {slideIndex_3 = 1}    
      if (n < 1) {slideIndex_3 = slides.length}
      for (i = 0; i < slides.length; i++) {
        slides[i].style.display = "none";  
      }
      for (i = 0; i < dots.length; i++) {
        dots[i].className = dots[i].className.replace(" active", "");
      }
      slides[slideIndex_3-1].style.display = "block";  
      dots[slideIndex_3-1].className += " active";
    }
    </script>
</body>
</html>
